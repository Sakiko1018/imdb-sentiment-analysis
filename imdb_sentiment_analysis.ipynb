{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMDB情感分析项目\n",
    "\n",
    "## 导入必要的库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据预处理类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMDBDataPreprocessor:\n",
    "    def __init__(self, max_len=256):\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def clean_text(self, text):\n",
    "        text = re.sub(r'<[^>]+>', '', text)\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "        return text.lower().strip()\n",
    "    \n",
    "    def prepare_glove_data(self, train_path, test_path, val_size=0.2):\n",
    "        train_df = pd.read_csv(train_path, sep='\\t', quoting=3)\n",
    "        test_df = pd.read_csv(test_path, sep='\\t', quoting=3)\n",
    "        \n",
    "        print(\"数据预处理中...\")\n",
    "        \n",
    "        # 构建词汇表\n",
    "        all_texts = train_df['review'].tolist() + test_df['review'].tolist()\n",
    "        word_to_idx = {'<pad>': 0, '<unk>': 1}\n",
    "        idx = 2\n",
    "        \n",
    "        for text in all_texts:\n",
    "            cleaned = self.clean_text(text)\n",
    "            for word in cleaned.split():\n",
    "                if word not in word_to_idx:\n",
    "                    word_to_idx[word] = idx\n",
    "                    idx += 1\n",
    "        \n",
    "        # 编码函数\n",
    "        def encode_texts(texts):\n",
    "            encoded = []\n",
    "            for text in texts:\n",
    "                cleaned = self.clean_text(text)\n",
    "                words = cleaned.split()[:self.max_len]\n",
    "                indices = [word_to_idx.get(word, 1) for word in words]\n",
    "                # 填充\n",
    "                if len(indices) < self.max_len:\n",
    "                    indices += [0] * (self.max_len - len(indices))\n",
    "                encoded.append(indices)\n",
    "            return torch.tensor(encoded, dtype=torch.long)\n",
    "        \n",
    "        # 编码数据\n",
    "        train_features = encode_texts(train_df['review'])\n",
    "        test_features = encode_texts(test_df['review'])\n",
    "        train_labels = torch.tensor(train_df['sentiment'].values, dtype=torch.long)\n",
    "        \n",
    "        # 分割验证集\n",
    "        train_features, val_features, train_labels, val_labels = train_test_split(\n",
    "            train_features, train_labels, test_size=val_size, random_state=42\n",
    "        )\n",
    "        \n",
    "        # 创建随机词向量（简化版）\n",
    "        vocab_size = len(word_to_idx)\n",
    "        weight_matrix = torch.randn(vocab_size, 300) * 0.1\n",
    "        \n",
    "        return {\n",
    "            'train': (train_features, train_labels),\n",
    "            'val': (val_features, val_labels),\n",
    "            'test': (test_features, test_df['id'].tolist()),\n",
    "            'word_to_idx': word_to_idx,\n",
    "            'weight_matrix': weight_matrix\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMAttentionModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_layers, num_classes, \n",
    "                 weight_matrix=None, bidirectional=True, dropout=0.3):\n",
    "        super(LSTMAttentionModel, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        if weight_matrix is not None:\n",
    "            self.embedding.weight.data.copy_(weight_matrix)\n",
    "        \n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers, \n",
    "                           batch_first=True, bidirectional=bidirectional, dropout=dropout)\n",
    "        \n",
    "        self.attention = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, 1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        lstm_out, _ = self.lstm(embedded)\n",
    "        \n",
    "        # 注意力机制\n",
    "        attention_weights = torch.softmax(self.attention(lstm_out).squeeze(-1), dim=1)\n",
    "        context_vector = torch.sum(attention_weights.unsqueeze(-1) * lstm_out, dim=1)\n",
    "        \n",
    "        output = self.dropout(context_vector)\n",
    "        output = self.fc(output)\n",
    "        \n",
    "        return output\n",
    "\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_classes, weight_matrix=None, \n",
    "                 num_filters=100, filter_sizes=[3,4,5], dropout=0.3):\n",
    "        super(CNNModel, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        if weight_matrix is not None:\n",
    "            self.embedding.weight.data.copy_(weight_matrix)\n",
    "        \n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv2d(1, num_filters, (fs, embed_dim)) for fs in filter_sizes\n",
    "        ])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(num_filters * len(filter_sizes), num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x).unsqueeze(1)\n",
    "        \n",
    "        conved = [nn.functional.relu(conv(embedded)).squeeze(3) for conv in self.convs]\n",
    "        pooled = [nn.functional.max_pool1d(conv, conv.size(2)).squeeze(2) for conv in conved]\n",
    "        \n",
    "        cat = self.dropout(torch.cat(pooled, dim=1))\n",
    "        output = self.fc(cat)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练器类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelTrainer:\n",
    "    def __init__(self, model, device, model_type='lstm'):\n",
    "        self.model = model.to(device)\n",
    "        self.device = device\n",
    "        self.model_type = model_type\n",
    "\n",
    "    def train(self, train_loader, val_loader, num_epochs=10, lr=0.001, patience=3):\n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), lr=lr)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        best_val_acc = 0\n",
    "        patience_counter = 0\n",
    "        best_model_state = None\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            self.model.train()\n",
    "            train_loss = 0\n",
    "            train_preds = []\n",
    "            train_labels = []\n",
    "\n",
    "            pbar = tqdm(train_loader, desc=f'Epoch {epoch + 1}/{num_epochs}')\n",
    "            for batch in pbar:\n",
    "                inputs, labels = batch\n",
    "                inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                logits = self.model(inputs)\n",
    "                loss = criterion(logits, labels)\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                train_loss += loss.item()\n",
    "                preds = torch.argmax(logits, dim=1)\n",
    "                train_preds.extend(preds.cpu().numpy())\n",
    "                train_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "                pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "\n",
    "            train_acc = accuracy_score(train_labels, train_preds)\n",
    "            val_acc = self.evaluate(val_loader)\n",
    "            avg_train_loss = train_loss / len(train_loader)\n",
    "\n",
    "            print(f'Epoch {epoch + 1}: Train Loss: {avg_train_loss:.4f}, Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}')\n",
    "\n",
    "            if val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "                patience_counter = 0\n",
    "                best_model_state = self.model.state_dict().copy()\n",
    "                torch.save(best_model_state, 'best_model.pth')\n",
    "                print(f\"保存最佳模型，验证准确率: {val_acc:.4f}\")\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= patience:\n",
    "                    print(f'早停于第 {epoch + 1} 轮')\n",
    "                    break\n",
    "\n",
    "        if best_model_state is not None:\n",
    "            self.model.load_state_dict(best_model_state)\n",
    "        else:\n",
    "            self.model.load_state_dict(torch.load('best_model.pth'))\n",
    "\n",
    "        return best_val_acc\n",
    "\n",
    "    def evaluate(self, data_loader):\n",
    "        self.model.eval()\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in data_loader:\n",
    "                inputs, labels = batch\n",
    "                inputs = inputs.to(self.device)\n",
    "                logits = self.model(inputs)\n",
    "                labels = labels.cpu().numpy()\n",
    "\n",
    "                preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "                all_preds.extend(preds)\n",
    "                all_labels.extend(labels)\n",
    "\n",
    "        return accuracy_score(all_labels, all_preds)\n",
    "\n",
    "    def predict(self, test_loader):\n",
    "        self.model.eval()\n",
    "        predictions = []\n",
    "\n",
    "        print(\"开始预测...\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i, batch in enumerate(test_loader):\n",
    "                if isinstance(batch, (list, tuple)):\n",
    "                    inputs = batch[0]\n",
    "                else:\n",
    "                    inputs = batch\n",
    "\n",
    "                inputs = inputs.to(self.device)\n",
    "                logits = self.model(inputs)\n",
    "                preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "                predictions.extend(preds)\n",
    "\n",
    "                if (i + 1) % 50 == 0:\n",
    "                    print(f\"已处理 {i + 1} 个batch，共 {len(predictions)} 条预测\")\n",
    "\n",
    "        print(f\"预测完成，共生成 {len(predictions)} 条预测结果\")\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 主执行流程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    print(\"IMDB情感分析开始...\")\n",
    "    \n",
    "    # 检查设备\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"使用设备: {device}\")\n",
    "    \n",
    "    # 数据路径\n",
    "    train_path = \"./corpus/imdb/labeledTrainData.tsv\"\n",
    "    test_path = \"./corpus/imdb/testData.tsv\"\n",
    "    \n",
    "    # 检查数据文件\n",
    "    if not os.path.exists(train_path):\n",
    "        print(\"错误：请确保数据文件在正确路径\")\n",
    "        print(\"需要文件: corpus/imdb/labeledTrainData.tsv 和 testData.tsv\")\n",
    "        return\n",
    "    \n",
    "    # 数据预处理\n",
    "    preprocessor = IMDBDataPreprocessor(max_len=256)\n",
    "    data = preprocessor.prepare_glove_data(train_path, test_path)\n",
    "    \n",
    "    # 创建数据加载器\n",
    "    train_dataset = TensorDataset(data['train'][0], data['train'][1])\n",
    "    val_dataset = TensorDataset(data['val'][0], data['val'][1])\n",
    "    test_dataset = TensorDataset(data['test'][0])\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "    \n",
    "    print(f\"数据统计: 训练样本 {len(train_dataset)}, 验证样本 {len(val_dataset)}, 测试样本 {len(test_dataset)}\")\n",
    "    \n",
    "    # 创建模型\n",
    "    vocab_size = len(data['word_to_idx'])\n",
    "    model = LSTMAttentionModel(\n",
    "        vocab_size=vocab_size,\n",
    "        embed_dim=300,\n",
    "        hidden_dim=128,\n",
    "        num_layers=2,\n",
    "        num_classes=2,\n",
    "        weight_matrix=data['weight_matrix'],\n",
    "        bidirectional=True\n",
    "    )\n",
    "    \n",
    "    # 训练模型\n",
    "    trainer = ModelTrainer(model, device)\n",
    "    best_acc = trainer.train(train_loader, val_loader, num_epochs=10)\n",
    "    \n",
    "    # 预测\n",
    "    predictions = trainer.predict(test_loader)\n",
    "    \n",
    "    # 保存结果\n",
    "    test_ids = data['test'][1]\n",
    "    result_df = pd.DataFrame({'id': test_ids, 'sentiment': predictions})\n",
    "    result_df.to_csv('imdb_predictions.csv', index=False)\n",
    "    \n",
    "    print(\"结果已保存到: imdb_predictions.csv\")\n",
    "    print(f\"最佳验证准确率: {best_acc:.4f}\")\n",
    "    print(\"项目执行完成\")\n",
    "\n",
    "# 运行主函数\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 运行代码\n",
    "\n",
    "执行下面的单元格来运行整个项目："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 运行主函数\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 结果展示\n",
    "\n",
    "运行完成后，查看预测结果："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 显示预测结果统计\n",
    "result_df = pd.read_csv('imdb_predictions.csv')\n",
    "print(\"预测结果统计:\")\n",
    "print(f\"总样本数: {len(result_df)}\")\n",
    "print(f\"正面评论比例: {result_df['sentiment'].mean():.2%}\")\n",
    "print(\"情感分布:\")\n",
    "print(result_df['sentiment'].value_counts().sort_index())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}